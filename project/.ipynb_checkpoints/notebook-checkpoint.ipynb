{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13f5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from net import Net\n",
    "from dataset import create_data_loader, split_train_val, getUpperIndices, addASpace\n",
    "\n",
    "import os\n",
    "import string\n",
    "\n",
    "#to do: \n",
    "    #shuffle test and train data\n",
    "    #see if vocab actually gets updated (it seems to be but i dont believe it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69155e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8f7e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#parameters:\n",
    "numEpochs = 10\n",
    "batchSize = 1\n",
    "learningRate = 0.001\n",
    "\n",
    "paddingValue = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print('device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fe342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['_PADDING_']\n",
    "\n",
    "\n",
    "# Turn string into tensor of ints\n",
    "def tensorConvert(string):\n",
    "    # runtime can be improved if we find a way to convert tensor using tensor / numpy methods of some sort\n",
    "    # only thing is i wouldnt know how to do this with a vocab though\n",
    "\n",
    "    output = []\n",
    "\n",
    "    tokenize = string.split()\n",
    "    for token in tokenize:\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "\n",
    "        output.append(vocab.index(token))\n",
    "        \n",
    "        #print('\\nappended',token,'to vocab!')\n",
    "\n",
    "    output = torch.LongTensor(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7830c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Classes: 50\n",
      "classes: ['AaronPressman', 'AlanCrosby', 'AlexanderSmith', 'BenjaminKangLim', 'BernardHickey', 'BradDorfman', 'DarrenSchuettler', 'DavidLawder', 'EdnaFernandes', 'EricAuchard', 'FumikoFujisaki', 'GrahamEarnshaw', 'HeatherScoffield', 'JaneMacartney', 'JanLopatka', 'JimGilchrist', 'JoeOrtiz', 'JohnMastrini', 'JonathanBirt', 'JoWinterbottom', 'KarlPenhaul', 'KeithWeir', 'KevinDrawbaugh', 'KevinMorrison', 'KirstinRidley', 'KouroshKarimkhany', 'LydiaZajc', \"LynneO'Donnell\", 'LynnleyBrowning', 'MarcelMichelson', 'MarkBendeich', 'MartinWolk', 'MatthewBunce', 'MichaelConnor', 'MureDickie', 'NickLouth', 'PatriciaCommins', 'PeterHumphrey', 'PierreTran', 'RobinSidel', 'RogerFillion', 'SamuelPerry', 'SarahDavison', 'ScottHillis', 'SimonCowell', 'TanEeLyn', 'TheresePoletti', 'TimFarrand', 'ToddNissen', 'WilliamKazer']\n"
     ]
    }
   ],
   "source": [
    "# get list of authors\n",
    "classes = []\n",
    "rootdir = 'Data/C50'\n",
    "for it in os.scandir(rootdir): #scan subdirectory and append each element to list of classes\n",
    "    if it.is_dir():\n",
    "        classes.append(it.path.replace(rootdir + \"\\\\\" , '')) #remove 'C50train\\' from string\n",
    "\n",
    "print('Num Classes:',len(classes))\n",
    "print('classes:',classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd639d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 5000\n"
     ]
    }
   ],
   "source": [
    "#data:\n",
    "length = sum([len(files) for r, d, files in os.walk(\"Data/C50\")])\n",
    "print('Number of files:', length)\n",
    "\n",
    "data = []\n",
    "\n",
    "def gatherData(path):\n",
    "    #0 = train, 1 = test\n",
    "    address = path + \"/C50\"\n",
    "\n",
    "        \n",
    "    prelude = len(address)\n",
    "    \n",
    "    for r, d, files in os.walk(address):\n",
    "        \n",
    "        #print here to show progress, warning, a shit load of print statements\n",
    "        \n",
    "        author = r[(prelude+1):]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if author == \"\":\n",
    "            continue\n",
    "        \n",
    "        #print('\\nr:',r)\n",
    "        print('\\nAuthor:', author)\n",
    "        ratio = classes.index(author) / len(classes)\n",
    "        print(ratio * 100, '% there.')\n",
    "            \n",
    "        #EARLY STOP FOR DEVELOPMENT PURPOSES (cuz going through every author takes an ass load time)\n",
    "        if author == \"DarrenSchuettler\":\n",
    "            break\n",
    "\n",
    "        for file in files:\n",
    "            address = r + '/' + file\n",
    "\n",
    "            with open(address, 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            convert = tensorConvert(content)\n",
    "            \n",
    "            #print('vocab length:',len(vocab))\n",
    "            #print('sample:',convert[:5])\n",
    "            \n",
    "            authorIdx = classes.index(author)\n",
    "            \n",
    "            item = [authorIdx, convert]\n",
    "            #print('item:',item)\n",
    "            \n",
    "            data.append(item)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6c8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author:  AaronPressman\n",
      "0.0 % there.\n",
      "\n",
      "Author:  AlanCrosby\n",
      "2.0 % there.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20932/1111864874.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgatherData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20932/1632321887.py\u001b[0m in \u001b[0;36mgatherData\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mconvert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m#print('vocab length:',len(vocab))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20932/22177134.py\u001b[0m in \u001b[0;36mtensorConvert\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print('\\nappended',token,'to vocab!')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gatherData('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f108eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b3f42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n",
      "1499\n"
     ]
    }
   ],
   "source": [
    "#find maxLength for padding purposes\n",
    "maxLength = 0\n",
    "\n",
    "for item in data:\n",
    "    tensor = item[1]\n",
    "    length = len(tensor)\n",
    "    if length > maxLength:\n",
    "        maxLength = length\n",
    "        \n",
    "print(maxLength)\n",
    "print(len(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb6426c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, tensor([ 0.,  1.,  2.,  ..., -1., -1., -1.])]\n"
     ]
    }
   ],
   "source": [
    "def padTensor(tensor):\n",
    "    length = len(tensor)\n",
    "\n",
    "    pad = torch.ones((maxLength - length,))\n",
    "    pad = pad * -1\n",
    "\n",
    "    newTensor = torch.cat((tensor, pad),0)\n",
    "    \n",
    "    return newTensor\n",
    "    \n",
    "for item in data:\n",
    "    tensor = item[1]\n",
    "    newTensor = padTensor(tensor)\n",
    "    \n",
    "    item[1] = newTensor\n",
    "\n",
    "item = data[0]\n",
    "print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59d08aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data: 5000\n",
      "Length of partitions: 4000 500 500\n",
      "Sum: 5000\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset\n",
    "trainData, valData, testData = split_train_val(data, props=[0.8, 0.1, 0.1])\n",
    "\n",
    "print('Length of data:', len(data))\n",
    "print('Length of partitions:',len(trainData), len(valData), len(testData))\n",
    "print('Sum:',len(trainData) + len(valData) + len(testData))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "081901f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[ 3.5150e+03,  2.3300e+02,  1.3900e+02,  ..., -1.0000e+00,\n",
       "          -1.0000e+00, -1.0000e+00]]),\n",
       " 'target': tensor([2])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader = create_data_loader(trainData, \n",
    "                                      batchSize,\n",
    "                                      shuffle=True)\n",
    "valLoader = create_data_loader(testData, \n",
    "                                    batchSize,\n",
    "                                    shuffle=True)\n",
    "\n",
    "next(iter(trainLoader)) #NOTE THAT THIS SHIT IS PACKAGED UP WATCH OUT HOPEFULLY NOT A BIG DEAL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "238bf0ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/2568176016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutputSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\GitHub\\cs4650_proj2\\project\\net.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, output_size, n_layers)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "#insert model here, need to get numbers for these\n",
    "inputSize = len(vocab)\n",
    "hiddenSize = 512\n",
    "outputSize = len(classes)\n",
    "\n",
    "net = Net(inputSize, hiddenSize, outputSize).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66e4f00b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/1017147377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\GitHub\\cs4650_proj2\\project\\net.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#print('output:',output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#start training + print accuracy\n",
    "#i see no reason to split any of this up into multiple cells but feel free to do so if there is one\n",
    "\n",
    "hidden = net.init_hidden()\n",
    "\n",
    "for e in range(numEpochs):\n",
    "    #training loop\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for item in trainLoader:\n",
    "        data = item['input']\n",
    "        label = item['target']\n",
    "        \n",
    "        data = data.long()\n",
    "\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = net(data, hidden)\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #validation loop\n",
    "    valid_loss = 0.0\n",
    "    net.eval()  # Optional when not using Model Specific layer\n",
    "    for item in validLoader:\n",
    "        data = item['input']\n",
    "        label = item['target']\n",
    "        \n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {e + 1} \\t\\t Training Loss: {train_loss / len(trainLoader)} \\t\\t Validation Loss: {valid_loss / len(validLoader)}')\n",
    "\n",
    "    #save model if validation loss decreases\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(net.state_dict(), 'Models/saved_model.pth')\n",
    "\n",
    "#test loop\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        data = item['input']\n",
    "        label = item['target']\n",
    "        outputs = net(data)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        if label == prediction:\n",
    "            correct_pred[classes[label]] += 1\n",
    "        total_pred[classes[label]] += 1\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cafb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
