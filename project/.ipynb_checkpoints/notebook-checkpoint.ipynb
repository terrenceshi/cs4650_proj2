{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13f5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from net import Net\n",
    "from dataset import create_data_loader\n",
    "\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5769c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8f7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters:\n",
    "numEpochs = 10\n",
    "batchSize = 1\n",
    "learningRate = 0.001\n",
    "\n",
    "paddingValue = -1 #atm its a negative number, edit vocab lst if you want to make it a positive number\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fe342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = string.printable\n",
    "\n",
    "vocab = []\n",
    "\n",
    "# Turn string into tensor of ints\n",
    "def tensorConvert(string): \n",
    "    #runtime can be improved if we find a way to convert tensor using tensor / numpy methods of some sort\n",
    "    #only thing is i wouldnt know how to do this with a vocab though\n",
    "    \n",
    "    output= []\n",
    "    \n",
    "    tokenize = string.split()\n",
    "    for token in tokenize:\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "        \n",
    "        output.append(vocab.index(token))\n",
    "        \n",
    "    output = torch.tensor(output)\n",
    "    \n",
    "    return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249b02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful function if you want to put a space between author names. since author names are written FirstnameLastname in the folders\n",
    "def getUpperIndices(s):\n",
    "    return [i for i, c in enumerate(s) if c.isupper()]\n",
    "\n",
    "def addASpace(s):\n",
    "    upIdx = getUpperIndices(s)[1]\n",
    "    s = s[:upIdx] + ' ' + s[upIdx:] #add space between author's first and last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9beb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['AaronPressman', 'AlanCrosby', 'AlexanderSmith', 'BenjaminKangLim', 'BernardHickey', 'BradDorfman', 'DarrenSchuettler', 'DavidLawder', 'EdnaFernandes', 'EricAuchard', 'FumikoFujisaki', 'GrahamEarnshaw', 'HeatherScoffield', 'JaneMacartney', 'JanLopatka', 'JimGilchrist', 'JoeOrtiz', 'JohnMastrini', 'JonathanBirt', 'JoWinterbottom', 'KarlPenhaul', 'KeithWeir', 'KevinDrawbaugh', 'KevinMorrison', 'KirstinRidley', 'KouroshKarimkhany', 'LydiaZajc', \"LynneO'Donnell\", 'LynnleyBrowning', 'MarcelMichelson', 'MarkBendeich', 'MartinWolk', 'MatthewBunce', 'MichaelConnor', 'MureDickie', 'NickLouth', 'PatriciaCommins', 'PeterHumphrey', 'PierreTran', 'RobinSidel', 'RogerFillion', 'SamuelPerry', 'SarahDavison', 'ScottHillis', 'SimonCowell', 'TanEeLyn', 'TheresePoletti', 'TimFarrand', 'ToddNissen', 'WilliamKazer']\n"
     ]
    }
   ],
   "source": [
    "# get list of authors\n",
    "classes = []\n",
    "rootdir = 'Data/C50train'\n",
    "for it in os.scandir(rootdir): #scan subdirectory and append each element to list of classes\n",
    "    if it.is_dir():\n",
    "        classes.append(it.path.replace(rootdir + \"\\\\\" , '')) #remove 'C50train\\' from string\n",
    "        \n",
    "print('classes:',classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd639d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 2500\n",
      "train: 2500\n"
     ]
    }
   ],
   "source": [
    "#data:\n",
    "testLen = sum([len(files) for r, d, files in os.walk(\"Data/C50test\")])\n",
    "print('test:', testLen)\n",
    "\n",
    "trainLen = sum([len(files) for r, d, files in os.walk(\"Data/C50train\")])\n",
    "print('train:', trainLen)\n",
    "\n",
    "trainData = []\n",
    "testData = []\n",
    "\n",
    "def gatherData(path, trainOrTest):\n",
    "    #0 = train, 1 = test\n",
    "    address = path + \"/C50train\"\n",
    "    \n",
    "    if trainOrTest == 1:\n",
    "        address = path + \"/C50test\"\n",
    "        \n",
    "    prelude = len(address)\n",
    "    \n",
    "    for r, d, files in os.walk(address):\n",
    "        \n",
    "        #print here to show progress, warning, a shit load of print statements\n",
    "        print('r:',r)\n",
    "        print('test:',prelude)\n",
    "        author = r[(prelude+1):]\n",
    "        print('\\nAuthor: ',author)\n",
    "        \n",
    "        if author == \"\":\n",
    "            continue\n",
    "            \n",
    "        #EARLY STOP FOR DEVELOPMENT PURPOSES (cuz going through every author takes an ass load time)\n",
    "        if author == \"AlanCrosby\":\n",
    "            break\n",
    "\n",
    "        for file in files:\n",
    "            address = r + '/' + file\n",
    "\n",
    "            with open(address, 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            convert = tensorConvert(content)\n",
    "            #print('sample:',convert[:5])\n",
    "            \n",
    "            authorIdx = classes.index(author)\n",
    "            \n",
    "            item = [authorIdx, convert]\n",
    "            #print('item:',item)\n",
    "            \n",
    "            if trainOrTest == 0:\n",
    "                trainData.append(item)\n",
    "            else:\n",
    "                testData.append(item)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6c8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: Data/C50train\n",
      "test: 13\n",
      "\n",
      "Author:  \n",
      "r: Data/C50train\\AaronPressman\n",
      "test: 13\n",
      "\n",
      "Author:  AaronPressman\n",
      "r: Data/C50train\\AlanCrosby\n",
      "test: 13\n",
      "\n",
      "Author:  AlanCrosby\n",
      "r: Data/C50test\n",
      "test: 12\n",
      "\n",
      "Author:  \n",
      "r: Data/C50test\\AaronPressman\n",
      "test: 12\n",
      "\n",
      "Author:  AaronPressman\n",
      "r: Data/C50test\\AlanCrosby\n",
      "test: 12\n",
      "\n",
      "Author:  AlanCrosby\n"
     ]
    }
   ],
   "source": [
    "gatherData('Data', 0)\n",
    "gatherData('Data', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f108eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,   0,  18,  19,  20,  21,  22,  23,  15,  24,  25,\n",
      "         26,  27,  15,   1,  28,  15,  29,  30,  10,  31,  32,  33,  10,  34,\n",
      "         35,  36,  37,  38,  39,   5,  40,  14,  41,  42,   0,  43,  34,  44,\n",
      "         45,  46,  47,  48,  49,  50,  51,  34,  52,  53,  54,  55,  27,  15,\n",
      "         56,  57,  49,  10,  58,  59,   1,  60,  61,  62,  63,   0,  64,  31,\n",
      "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "         79,  80,  34,  26,  27,  15,   1,  81,  82,  83,  21,  84,  85,  86,\n",
      "         14,  15,  87,  88,  89,  90,  14,  91,  92,   1,  60,  61,  93,  71,\n",
      "         34,  94,  95,  96,  15,  97,  10,  98,  99,  50,  10, 100, 101, 102,\n",
      "        103, 104, 105,  15, 106, 107,   1,  66,  96, 108, 109,  34,  52, 110,\n",
      "         74, 111, 112,  23, 113, 114,  10, 115, 116, 117, 118,  33, 119, 120,\n",
      "        121, 122,  34, 123,  68, 124, 125, 126,  14, 127, 128, 111, 129,  24,\n",
      "         14,  15, 130,  15, 106, 131,  60,  66,  68,  15, 132,  64,  31,  70,\n",
      "         71, 133, 115, 134, 135,  37, 136,  96, 137, 138, 139, 140, 141,   0,\n",
      "        142,   1, 143,  15, 144, 145,  28,  15, 146,  14,  35,   1, 147, 148,\n",
      "        149, 150, 151,  49, 152, 153,   1, 154, 155, 102, 156, 157, 158, 159,\n",
      "         79, 160, 161, 162, 148, 149, 163, 164, 153, 165, 166, 115,  15, 167,\n",
      "        139, 168, 169, 170, 171, 172, 173, 153, 174,  62, 175,  15, 144, 176,\n",
      "        177, 178,  50, 179, 180, 181, 182, 183,   2, 184, 185,  10,  34, 180,\n",
      "        153, 186, 187, 188, 189, 153, 190, 191,  25, 192, 193,  96, 194, 195,\n",
      "        196, 197, 198, 199,  62, 200,   0,  20, 201, 202, 203,  23, 204, 113,\n",
      "        205, 206,  96, 207,  34, 208,  14,  15,  60,  61, 209])]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3f42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048\n",
      "319\n"
     ]
    }
   ],
   "source": [
    "#find maxLength for padding purposes\n",
    "maxLength = 0\n",
    "\n",
    "for item in trainData:\n",
    "    tensor = item[1]\n",
    "    length = len(tensor)\n",
    "    if length > maxLength:\n",
    "        maxLength = length\n",
    "        \n",
    "for item in testData:\n",
    "    tensor = item[1]\n",
    "    length = len(tensor)\n",
    "    if length > maxLength:\n",
    "        maxLength = length\n",
    "        \n",
    "print(maxLength)\n",
    "print(len(trainData[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb6426c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, tensor([ 0.,  1.,  2.,  ..., -1., -1., -1.])]\n"
     ]
    }
   ],
   "source": [
    "def padTensor(tensor):\n",
    "    length = len(tensor)\n",
    "\n",
    "    pad = torch.ones((maxLength - length,))\n",
    "    pad = pad * -1\n",
    "\n",
    "    newTensor = torch.cat((tensor, pad),0)\n",
    "    \n",
    "    return newTensor\n",
    "    \n",
    "for item in trainData:\n",
    "    tensor = item[1]\n",
    "    newTensor = padTensor(tensor)\n",
    "    \n",
    "    item[1] = newTensor\n",
    "        \n",
    "for item in testData:\n",
    "    tensor = item[1]\n",
    "    newTensor = padTensor(tensor)\n",
    "    \n",
    "    item[1] = newTensor\n",
    "\n",
    "item = trainData[0]\n",
    "print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4394f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[489., 680., 321.,  ...,  -1.,  -1.,  -1.]]),\n",
       " 'target': tensor([0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader = create_data_loader(trainData, \n",
    "                                      batchSize,\n",
    "                                      shuffle=True)\n",
    "valLoader = create_data_loader(testData, \n",
    "                                    batchSize,\n",
    "                                    shuffle=True)\n",
    "\n",
    "next(iter(train_dataloader)) #NOTE THAT THIS SHIT IS PACKAGED UP WATCH OUT HOPEFULLY NOT A BIG DEAL\n",
    "\n",
    "#to do: \n",
    "    #shuffle test and train data\n",
    "\n",
    "    #allocate half the test data into validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238bf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert model here, need to get numbers for these\n",
    "inputSize = 1\n",
    "hiddenSize = 1\n",
    "outputSize = len(classes)\n",
    "\n",
    "net = Net(inputSize, hiddenSize, outputSize)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start training + print accuracy\n",
    "#i see no reason to split any of this up into multiple cells but feel free to do so if there is one\n",
    "\n",
    "for e in range(numEpochs):\n",
    "    #training loop\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for data, labels in trainLoader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        target = net(data)\n",
    "\n",
    "        loss = criterion(target, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #validation loop\n",
    "    valid_loss = 0.0\n",
    "    net.eval()  # Optional when not using Model Specific layer\n",
    "    for data, labels in validLoader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        target = net(data)\n",
    "        loss = criterion(target, labels)\n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {e + 1} \\t\\t Training Loss: {train_loss / len(trainLoader)} \\t\\t Validation Loss: {valid_loss / len(validLoader)}')\n",
    "\n",
    "    #save model if validation loss decreases\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(net.state_dict(), 'Models/saved_model.pth')\n",
    "\n",
    "#test loop\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
